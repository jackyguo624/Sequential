# pytorch_lightning==2.5.1
seed_everything: true
model:
  # class_path: models.asr.dummy.models.dnn_manual.Dnn
  # init_args:
  #   input_dim: 80
  #   hidden_dim: 100
  #   output_dim: 80

  class_path: models.asr.fireredasr.models.fireredasr_llm.FireRedAsrLlm
  init_args:
    encoder:
      class_path: models.asr.fireredasr.models.module.conformer_encoder.ConformerEncoder
      init_args:
        idim: 80
        n_layers: 16
        n_head: 20
        d_model: 1280
        residual_dropout: 0.1
        dropout_rate: 0.1
        kernel_size: 33
        pe_maxlen: 5000

    llm:
      class_path: sequential.utils.builder.auto_model_for_causal_lm_from_pretrained_wrapper
      init_args:
        pretrained_model_name_or_path: models/asr/fireredasr/pretrained_model/FireRedASR/FireRedASR-LLM-L/Qwen2-7B-Instruct
      dict_kwargs:
        attn_implementation: eager # flash_attention_2
        torch_dtype: torch.float16

    tokenizer:
      class_path: models.asr.fireredasr.tokenizer.llm_tokenizer.build_llm_tokenizer
      init_args:
        llm_path: models/asr/fireredasr/pretrained_model/FireRedASR/FireRedASR-LLM-L/Qwen2-7B-Instruct

    encoder_projector:
      class_path: models.asr.fireredasr.models.module.adapter.Adapter
      init_args:
        encoder_dim: 1280
        llm_dim: 3584
        downsample_rate: 2

    freeze_encoder: false
    freeze_llm: false
    use_lora: true
    model_path: models/asr/fireredasr/pretrained_model/FireRedASR/FireRedASR-LLM-L/model.pth.tar

data:
  class_path: sequential.data.lhotse_datamodule.LhotseManifestDataModule
  init_args:
    train_manifest_dir: export/aispeech/asr/collectionzh+en+fangyan-2w-asr/train1000
    train_dataset:
      class_path: sequential.data.dataset.K2SpeechRecognitionDatasetWraper
      init_args:
        input_strategy:
          class_path: lhotse.dataset.input_strategies.OnTheFlyFeatures
          init_args:
            extractor:
              class_path: models.asr.fireredasr.data.asr_feat.FiredasrFbank
              init_args:
                kaldi_cmvn_file: models/asr/fireredasr/pretrained_model/FireRedASR/FireRedASR-LLM-L/cmvn.ark
    train_sampler:
      class_path: lhotse.dataset.sampling.SimpleCutSampler
      # class_path: sequential.data.sampler.SimpleSamplerWraper
      init_args:
        max_duration: 50
        shuffle: True
    audio_backend:
      class_path: sequential.data.audio_backend.KaldiAudioBackend
      init_args:
        force_sampling_rate: 16000


    val_manifest_dir: export/aispeech/asr/collectionzh+en+fangyan-2w-asr/dev500
    val_dataset:
      class_path: sequential.data.dataset.K2SpeechRecognitionDatasetWraper
      init_args:
        input_strategy:
          class_path: lhotse.dataset.input_strategies.OnTheFlyFeatures
          init_args:
            extractor:
              class_path: models.asr.fireredasr.data.asr_feat.FiredasrFbank
              init_args:
                kaldi_cmvn_file: models/asr/fireredasr/pretrained_model/FireRedASR/FireRedASR-LLM-L/cmvn.ark
    val_sampler:
      class_path: lhotse.dataset.sampling.SimpleCutSampler
      # class_path: sequential.data.sampler.SimpleSamplerWraper
      init_args:
        max_duration: 50
        shuffle: False
    audio_backend:
      class_path: sequential.data.audio_backend.KaldiAudioBackend
      init_args:
        force_sampling_rate: 16000

trainer:
  default_root_dir: fireredasr_npu_deepspeed_data1000
  max_epochs: 5
  logger: true
  accelerator: 
    class_path: lightning_npu.accelerators.npu.NPUAccelerator
  devices: 1
  num_nodes: 1
  strategy: 
    class_path: lightning_npu.strategies.npu_deepspeed.NPUDeepSpeedStrategy
    init_args:
      config: models/asr/dummy/configs/ds_config.json
  use_distributed_sampler: false
  callbacks:
    - class_path: pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint
      init_args:
        dirpath: fireredasr_npu_deepspeed_data1000/checkpoints
        monitor: val_loss
        mode: min
        every_n_train_steps: 50
        save_on_train_epoch_end: true
        save_top_k: 2
        filename: "{epoch}-{step}-{val_loss:.4f}"

